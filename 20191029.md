# Crawler VS Scraping





## Crawler

: 크롤러는 **조직적, 자동화**된 방법으로 월드 와이드 **웹을 탐색**하는 컴퓨터 프로그램이다. 크롤러는 크롤링 혹은 스파이더링 이라는 작업을 웹상에서 진행하게 된다.  크롤러는 방문한 사이트의 모든 페이지의 복사본을 생성하는 데 사용되며, 검색 엔진은 이렇게 생성된 페이지를 보다 빠른 검색을 위해 인덱싱을 하게 된다.  또한 크롤러는 링크 체크나 HTML 코드 검증과 같은 웹 사이트의 자동 유지 관리 작업을 위해서 사용되기도 한다. 또한 자동 이메일 수집과 같은 웹페이지의 특정 형태의 정보를 수집하는 데도 사용된다. 

하지만 각 사이트마다 개인의 크롤러 사용에 대한 정책은 다르다. 다음, 네이버 같이 나름(?) 편하게(1000개) 할 수 있도록 풀어주는 곳도 있다. 반대로 구글의 경우에는 구글링을 하기에 상당히 까다롭다. 검색어 API 14년도 이후로 폐쇄를 했고, 구글에서 들어가는 기사나 페이지 들의 URL을 계속해서 바꿔주기 때문에, 아직 나는 수동으로만 바꾸는 능력이 있어서 매우 까다로웠다. 



## Scraping

: 컴퓨터 프로그램이 다른 프로그램으로부터 들어오는 인간이 읽을 수 있는 출력으로부터 데이터를 추출하는 기법이다. 일반적으로 프로그램들 간의 데이터 전송은 인간이 아닌 컴퓨터에 맞는 자동화 시스템에서 맞는 구조로 돌아가게 된다. 컴퓨터를 위해서는 아주 확실하지만, 종종 인간이 읽기에는 쉽지 않은 경우가 있다.



그래서 그러므로 데이터 스크래핑을 정규적인 [구문 분석](https://ko.wikipedia.org/wiki/구문_분석)과 구별하는 주요 요소는 "스크래핑"이 되는 출력물이 [최종 사용자](https://ko.wikipedia.org/wiki/최종_사용자)에게 표시할 것을 염두에 둔다는 점이다. (다른 프로그램에게 입력을 전달하는 것이 아니라) 그러므로 편리한 구문 분석을 위해 문서화/구조화가 되어 있지 않을 수 있다. 데이터 스크래핑은 바이너리 데이터(일반적으로 이미지나 멀티미디어 데이터), [디스플레이](https://ko.wikipedia.org/wiki/디스플레이_장치) 서식 지정, 과잉 레이블, 과도한 주석, 자동화된 처리를 저해하는 기타 정보를 수반하기도 한다. 



> 참고: 위키피디아